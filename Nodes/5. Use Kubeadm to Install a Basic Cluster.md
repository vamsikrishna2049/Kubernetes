Now that we have the nodes ready with all the utilities for kubernetes, we will initialize the control plane. We will be using Kubernetes version v1.30

#Important Note:
If you're using cloud VMs with public IP addresses, it's important to assign a static public IP to the control plane node if you plan to access the cluster remotely from your workstation. Without a static IP, if the VMs are shut down and the IP changes, the cluster may not function as expected.


Create a file named ```kubeadm.config``` on the control plane node with the following content. 

```bash
vi kubeadm.config
```

In the following YAML, replace 192.168.201.10 with the Private IP of your control plane node (advertiseAddress & certSANs).

If you are using cloud VMs with public IP, use the public IP in the controlPlaneEndpoint parameter

```yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "192.168.201.10"
  bindPort: 6443
nodeRegistration:
  name: "controlplane"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.30.0
controlPlaneEndpoint: "192.168.201.10:6443"
apiServer:
  certSANs:
    - 127.0.0.1
    - 192.168.201.10
  extraArgs:
    bind-address: "0.0.0.0"
scheduler:
  extraArgs:
    bind-address: "0.0.0.0"
controllerManager:
  extraArgs:
    bind-address: "0.0.0.0"
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/12"
```

Execute the following command to initialize the cluster.

Ensure you execute the command using **sudo**

```yaml
sudo kubeadm init --config=kubeadm.config
```
Upon successful execution, you will see three command sections in the output, as shown in the image below:

1. Command to copy the admin kubeconfig file.
2. Command to join other control plane nodes (not required for this guide).
3. Command to join the worker nodes.

![kubeconfig](https://raw.githubusercontent.com/vamsikrishna2049/Kubernetes/main/Nodes/images/kube-config.png)

Copy the node join command to Notepad, as it will be required to join the worker nodes to the master in the next topic.
Execute the following command to add the admin kubeconfig file to the home folder, allowing you to use the kubectl command with the cluster. If you plan to use the vagrant user, first switch to the vagrant or relevant user, and then run the commands.

```bash
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

Validate the cluster control plane using the following command. Ensure you execute this command from the control plane node where you copied the kubeconfig file in the previous step.

```bash
kubectl get pods --all-namespaces
```

You should get an output as shown below with all the control plane pod in ready and running state.

```bash
kubectl get pods --all-namespaces

NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE
kube-system   coredns-7db6d8ff4d-6btvf               1/1     Running   0          11m
kube-system   coredns-7db6d8ff4d-rp5vc               1/1     Running   0          11m
kube-system   etcd-controlplane                      1/1     Running   0          11m
kube-system   kube-apiserver-controlplane            1/1     Running   0          11m
kube-system   kube-controller-manager-controlplane   1/1     Running   0          11m
kube-system   kube-proxy-jhspz                       1/1     Running   0          11m
kube-system   kube-scheduler-controlplane            1/1     Running   0          11m
```

### Important Note: 
Always remember that kubectl is configured to be used from the control plane. Therefore, always execute kubectl commands from the control plane node. If you run them on other nodes, you will encounter the following "connection refused" error.

