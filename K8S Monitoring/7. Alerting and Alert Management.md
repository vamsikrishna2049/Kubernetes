# Alerting and Alert Management

## Alerting within Prometheus
Prometheus is capable of generating alerts when activity exceeds set thresholds. How does this work?

A dimensioned or labeled data model is in use with Prometheus, meaning there’s a metric name and a set of metric labels that can be used to filter the collected time series data. 

You’re also able to reference labels within alert field templates using variables like {{$labels.job}}. These should all be written with a minimal footprint given that each template is executed for every rule iteration for each alert that occurs (resulting in potentially high impacts to performance and costs). Additionally, the use of labels should be restricted for only metrics that require labels to control costs and performance, as each label set requires a dedicated set of resources (including RAM, CPU, storage, and bandwidth). For more information, check the documentation for Prometheus template examples.

## Basic alerting
Alerts in Prometheus follow this basic structure:

```bash
<metricName>{<labelName>="<labelValue>", ...} <operator> <value>
```

where metricName is the Prometheus metric being queried, labelName optionally specifies the name of the label (and it’s corresponding label) that should be used to filter the query, the operator is the mathematical operator used to filter the results (such as ==, >, or <), and value is the comparison value for the query.

## Example

You can create alerts that are based on the metric label or part of the metric label that will stay constant for accurate and consistent targeting. As an example, on its own, the following PromQL expression will return the number of NGINX connections that were dropped.

```bash
nginxplus_connections_dropped{instance="exampleInstance",service="exampleService"}
```
When writing up an alert based on this expression, for instance, you can set an alert to fire when a rate of more than 20 dropped connections per second is reported.

```bash
rate(nginxplus_connections_dropped{service="myService"})[5m] > 20
```
 
## Advanced alerting
Additionally, you can take advantage of the powerful feature of aggregation operators which allows you to aggregate data results with operators like sum, min, max, avg, group, and count. These can be used across all label dimensions or across a filtered set using the clauses without or by, used before or after the query.

These aggregate metrics that are collected in order to achieve more complex comparisons. 

### Example

To trigger an alert if the average rate of total HTTP requests per second exceeds 50 requests in a five minute period, you would use the following PromQL query:

```bash
avg(rate(http_requests_total{service="api"}[5m])) > 50
```
 
### Caveats
It’s important to note that Prometheus itself—while great at metrics collection—is not designed for alerting, data processing or routing. As such, it by itself does not provide a complete developer experience for auditing, thresholds, or triggers and usually needs to be complemented by additional solutions in order to contribute to a complete observability stack.

## Additional resources
There’s a host of available resources online for building out Prometheus rules that are suitable for just about any use case. 

One fantastic link at the time of writing this course is this open source collection of Prometheus alerting rules called Awesome Prometheus Alerts. 

Another is promgen, designed to help you create and manage Prometheus configuration files, configure alert rules, and set up notifications. 

One of the best perks about open source is the wide range of support and references available from the greater community—make use of it!

### Alerting within Alertmanager
The development team behind Prometheus created an officially supported and open source tool called Alertmanager that can be used for efficient alerting and alert management. While it is optional to install, it is highly recommended to use alongside Prometheus for streamlined development. 

This is because the goal of this tool is to ensure you can optimize alerting data the moment it becomes available. Using Alertmanager, alerts are appropriately and quickly aggregated allowing development teams to efficiently manage alerting without redundant or unnecessary information. 

It’s also meant to route alerts and send out notifications effectively to a wide range of different receivers. Alerts are able to be sent to email, Slack, PagerDuty, OpsGenie, Telegram, Microsoft Teams, WebEx, Discord, and a number of other receivers. For notification mechanisms that aren't supported out-of-the-box, there's a webhook system that fully supports custom integrations. 

Because it comes with native support for using Prometheus (PromQL) rules and expressions for alerting, many observability stacks that are centered around Prometheus as the primary metrics collector work best with Alertmanager. Because of the lack of overhead required to power data visualizations, a streamlined Prometheus and Alertmanager stack is additionally able to handle alerting at a much larger scale compared to Grafana or other metric visualization solutions.

With that being said, it can be used alongside other solutions like Grafana without an issue (and in fact, is commonly combined with these other solutions as part of a complete observability stack). This way, alerting is able to be handled by a dedicated Prometheus-centered manager while other aspects of the stack can focus on their dedicated tasks (such as visualization or logging). 

#### Note:
For additional reference regarding installation, configuration, and usage of Alertmanager, consult the official Prometheus documentation.


## Alerting within Grafana (and other third-party tools)
While taking note of the considerations mentioned in the previous section, it’s still important to know that any and all time series that are imported into Grafana and similar solutions are able to use this additional layer of observability to handle alerting as well.

This approach is typically best for smaller setups or for handling data sources outside of Prometheus. Full integration with a wide range of receivers including email, PagerDuty, and Slack is also supported. 

If you’re already using these technologies within your alerting stack, it can be helpful to centralize monitoring efforts alongside alerting and manage it all from one place. There are also other benefits, like Grafana itself has a reputation of making the setup and configuration process for alerting a much more positive developer experience as opposed to Alertmanager. However, they shouldn’t be considered a complete solution on their own (especially if your project requires alerting at scale), and should instead be considered just one part of the overall approach.


With Grafana, this consideration is fully thought out as part of the features powering the solution as you’re able to configure an external Alertmanager to receive all alerts. This is then configured and administered from within Grafana for completely centralized alert management. Data sources that are supported at the time of writing this course are Prometheus, Grafana Mimir, and Cortex.
