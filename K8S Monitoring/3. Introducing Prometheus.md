# Introducing Prometheus
Now that we’ve covered the basics, it’s time to discuss the main subject of this course, Prometheus. In this chapter, we'll cover: 

1. What Prometheus does
2. What the main features are
3. The different components involved
4. What the architecture looks like and how it works
5. The use cases to consider when deciding to use or not use Prometheus versus other solutions

## What is Prometheus, anyway?
Prometheus, in a nutshell, is an open-source toolkit written in Go that's designed to be a fully featured solution for monitoring and alerting. It’s been around for more than ten years and has maintained a consistent userbase over that time.

It’s part of the Cloud Native Computing Foundation (CNCF), and was the second to join following Kubernetes in 2016. The Prometheus community has seen rapid and widespread adoption since the beginning and this trend has continued on till today. In [Grafana Lab’s 2024 Observability Survey](https://grafana.com/observability-survey/) (the most recent at the time of this course’s creation) three-quarters of survey respondents stated they’re using Prometheus in production today, and another 14% are looking into it. 

It's a highly popular choice due to its inherent reliability, flexibility, and performance as well as the capability of monitoring everything from dynamic containerized environments to traditional or static infrastructure.

Prometheus is responsible for collecting metrics data and storing it in an efficient time-series database, while allowing users to query that time-series data and configure alerting for real-time updates. It actually will use the built-in database for this purpose regardless of whether you forward metrics to an external storage platform or keep everything within Prometheus.

## Features
There are several features that should be highlighted in Prometheus.

First, it uses a **multidimensional data model** that accommodates time-series data. This data is associated with the timestamp and optional key-value pairs. 

There is also a native functional query language called **PromQL** that allows users to design powerful and streamlined queries and execute them against their time-series data.

You get **built-in visualization** options including a built-in web console and full support for third-party tooling.

To **retrieve stored time-series data**, Prometheus uses a HTTP pull model. You're also able to **push data directly to or from Prometheus** using an intermediary gateway like the officially supported and maintained [Prometheus Pushgateway](https://github.com/prometheus/pushgateway).

**Targets are discoverable** via powerful service discovery implementations (including Kubernetes, DigitalOcean, HTTP, and file-based as only a [few of the options available](https://github.com/prometheus/prometheus/tree/main/discovery)) or static configuration.

**Scaling** is handled differently in Prometheus than in other modern technologies; it specifically doesn’t scale horizontally. This means you can have many Prometheus instances that are all independent of each other with no cross communication. Putting it in perspective, if you have many Prometheus instances, each would need to be configured as a unique data source within Grafana. Note: To address this, you can export system metrics to an external storage platform that allows for horizontal scaling like Thanos or Cortex.

And of course, you're able to configure alerting based on alerting rules and send notifications across different platforms. This is essential for staying on top of suspicious, costly, or unexpected issues and activity.

## Components
Prometheus itself consists of quite a few different components.

The **Prometheus server** is responsible for collecting time-series data from exporters or scraping data from target systems and then storing this data in a time-series database.

**Various client libraries are supported** for programming languages including Rust, Python, Java, Ruby, and Go, along with many other third-party client libraries. These libraries aid in instrumenting application code. **Special exporters are also supported** for exposing metrics from systems that cannot directly use Prometheus metrics, such as Graphite, StatsD, and others including third-party software. 

For handling alerting within Prometheus, **Alertmanager** is used.

**Service discovery** mechanisms such as Kubernetes' native service discovery, DNS, and file_sd are supported to discover and begin monitoring new targets automatically.

The **push gateway** feature is a separate component here and it is used to collect metrics from short-lived jobs that cannot be scraped by usual automatic methods. 

And, **PromQL** is of course an additional component here used to provide a built-in expressive query language for querying and aggregating time-series data within Prometheus.

## Architecture
The image is a diagram representing the architecture of Prometheus, a monitoring and alerting system.  Short-lived Jobs: Depicted on the top left, these jobs push metrics to the Pushgateway at exit. Pushgateway: This component is shown in the center-left and is responsible for pushing metrics to the Prometheus server. Jobs/Exporters: Located at the bottom left, they pull metrics from various nodes. Service Discovery: Positioned at the top center, it includes Kubernetes and file_sd for discovering targets. Prometheus Server: Illustrated in the middle, it consists of three main components: Retrieval: Pulls metrics from the Pushgateway and jobs/exporters. TSDB: Time Series Database, stores the metrics. HTTP Server: Serves the metrics. Node: Shown at the bottom, it indicates where data is stored on HDD/SSD. Alertmanager: On the right side, it pushes alerts to notification systems like PagerDuty, Email, etc. Prometheus Web UI: Located at the bottom right, provides a user interface for viewing metrics. Grafana and API Clients: These are also at the bottom right, used for visualizing metrics. The diagram shows the flow of data from metric sources to storage and visualization, as well as alert management.

 

 

**Source credit:** https://prometheus.io/docs/introduction/overview/#architecture

For reference, let's look at this architecture diagram from the Prometheus documentation. 

We've got the short-lived jobs being pushed to the push gateway. The Prometheus server then pulls metrics from the Prometheus targets and the gateway. 

During the retrieval stage, the Prometheus server also connects to service discovery to discover the targets and moves on to pushing time series data to a storage volume. 

Meanwhile, the server is pushing alerts to Alertmanager which is handling configured alerts being pushed out to different channels like PagerDuty, email, and others. 

Any services handling data visualization and exporting like the Prometheus built-in UI or third-party tools like Grafana along with any other API clients also will interact using the information. 

Both PromQL and Alertmanager interact with the Prometheus server through the HTTP server that exposes data for use.

## When to use it
Prometheus is helpful to use in a variety of use cases, including application performance or load monitoring using metrics such as resource usage and latency, analytics for detecting patterns and trends with real-time monitoring, ensuring SLA times are being met when providing or receiving services, and infrastructure monitoring for insights into the general system health. It's especially popular when used to collect numeric metrics from a service that runs 24/7. 

It's really overall a solution made for microservices and containerized architectures, cloud-native environments, and situations requiring high reliability. Prometheus prides itself on being reliable to the point that metrics can be viewed even under failure conditions, independently from network storage or other services. 

It can be also used for many other use cases of course, however, it's important as well to know when it's not going to be the right solution for your needs. 

## When not to use it
It's recommended that you use a different system for use cases that require high accuracy, such as billing. Prometheus is meant for operational monitoring where minor inaccuracies are inevitable due to failed scrapings or kernel planning. Collected data is not likely to be detailed nor complete enough for these use cases.

It also is not capable of logging or auditing events, so you'll need a separate solution to achieve this aspect of observability within your system.

A separate solution is also needed to archive historical data for analysis over a longer period of time such as a few months or a year to obtain better insights into their system

Distributed tracing is not supported for identifying all operations that were triggered by a single request. This lack can be alleviated by using an end-to-end distributed tracing tool such as [Jaeger](https://www.jaegertracing.io/).

### Wondering about the alternatives that are available? 
Prometheus actually writes up a comparison to some significant metrics collection options in the [official docs](https://prometheus.io/docs/introduction/comparison/).
